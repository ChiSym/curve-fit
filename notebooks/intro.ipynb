{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth  # pyright: ignore [reportMissingImports]\n",
    "\n",
    "    auth.authenticate_user()\n",
    "    %pip install --quiet keyring keyrings.google-artifactregistry-auth   # type: ignore # noqa\n",
    "    %pip install --quiet genjax==0.4.0.post4.dev0+9d775c6f genstudio==v2024.06.20.1130 genjax-blocks==0.1.0 --extra-index-url https://us-west1-python.pkg.dev/probcomp-caliban/probcomp/simple/  # noqa # type: ignore\n",
    "    # This example will work on GPU, CPU or TPU. To change your runtime,\n",
    "    # select \"Change runtime type\" from the dropdown on the top right\n",
    "    # of the colab page.\n",
    "    #\n",
    "    # Make sure that the string in brackets below is either `cuda12` (for GPU), `cpu` or `tpu`:\n",
    "    %pip install --quiet jax[cpu]==0.4.28  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# DSL for curve fit inference\n",
    "\n",
    "As part of the `genjax.interpreted` performance investigation, I wanted to investigate a DSL for the curve-fitting task which could achieve JAX-accelerated performance without introducing JAX concepts such as the Switch and Map combinators, `in_axes`, and other things that might complicate the exposition of inference for the newcomer. While doing so, I also studied ways to \"automatically\" thread randomness through the computation without having to discuss the careful use of `jax.random.split` which we recommend to GenJAX users. Having done the experiment, I have mixed feelings about the results: on the one hand, it is possible to get JAX-level performance with curve-building combinators, but the price of so doing is that the GFI is hidden from view as well, and that may be too far a step. Nonetheless, if you're still interested in what can be achieved in this framework, read on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import genjax\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax.typing import PRNGKey, FloatArray, ArrayLike\n",
    "import genjax_blocks as b\n",
    "import genstudio.plot as Plot\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "genjax.pretty()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "The `blocks` library is concentrated on a simple $\\mathbb{R}\\rightarrow\\mathbb{R}$ inference problem, localized to the unit square for convenience, as found in the introductory GenJAX notebooks. We provide a \"basis\" of polynomial, $ae^{bx}$, and $a\\sin(\\phi + 2\\pi x/T)$ functions, each of whose parameters are drawn from a standard distribution that the user supplies. The intro curve fit task contemplates a polynomial of degree 2 with normally distributed coefficients, which we may write as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic = b.Polynomial(max_degree=2, coefficient_d=genjax.normal(0.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Note one felicity we have achieved already: we can specify the normal distribution in a simple fashion without having to tuple the arguments or provide randomness: that comes later. The type of `P` is `Block`, a name inspired by [blockly](https://developers.google.com/blockly). The only operation we can do on a Block is request an array of samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = quadratic.sample()\n",
    "p.get_choices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "The trace contains choices and a return value. In the blocks DSL, the return value is a JAX-compatible native Python function representing the sampled polynomial $x\\mapsto p(x)$. Calling it with the value 0.0 will reveal the constant term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.get_retval()(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "You may have noticed an extra layer of brackets around the polynomial's coefficients and its return value. This is the magic of JAX: sampling from a block produces an \"array BlockFunction\". Such an array of functions, when presented with an argument, will return an array of values (one for each function evaluated at that single argument.)\n",
    "\n",
    "Here's a simple function that will plot a bundle of samples from a block. In this case, we will want to provide a vector of arguments to each function (points along the $x$ axis). Since BlockFunctions are JAX-compatible, we can use `jax.vmap` for that. Now, the functions which already knew how to act as a vector given one argument can act that way across a vector of arguments, producing a grid of points, all in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_functions(fns: b.BlockFunction, **kwargs):\n",
    "    xs = jnp.linspace(-1, 1, 200)\n",
    "    yss = jax.vmap(fns)(xs)\n",
    "    return Plot.new(\n",
    "        [\n",
    "            Plot.line({\"x\": xs, \"y\": ys, \"stroke\": i % 12}, kwargs)\n",
    "            for i, ys in enumerate(yss.T)\n",
    "        ],\n",
    "        {\"clip\": True, \"height\": 400, \"width\": 400, \"y\": {\"domain\": [-1, 1]}},\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_priors(B: b.Block, n: int):\n",
    "    return plot_functions(B.sample(n).get_retval())\n",
    "\n",
    "\n",
    "plot_priors(quadratic, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "We can do the same for our Periodic and Exponential distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "periodic = b.Periodic(\n",
    "    amplitude=genjax.beta(2.0, 5.0),\n",
    "    phase=genjax.uniform(-1.0, 1.0),\n",
    "    period=genjax.normal(1.0, 1.0),\n",
    ")\n",
    "\n",
    "exponential = b.Exponential(a=genjax.normal(0.0, 1.0), b=genjax.normal(0.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_priors(periodic, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_priors(exponential, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "`Block` objects support the pointwise operations $+, *$ as well as `@` (function composition). Vikash's favorite ascending-periodic function might be modeled by the sum of a polynomial and a periodic function which we can write simply as $P+Q$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_priors(quadratic + periodic, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "It does seem like the goal function lies in the span of the prior distribution in this case. (I pause here to note that pointwise binary operations in `Block` are not commutative as you might expect, because the randomness supplied by `sample` is injected left to right).\n",
    "\n",
    "The binary operations produce traces representing the expression tree that created them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "(quadratic + periodic).sample().get_choices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Having assembled these pieces, let's turn to the inference task. This begins with a set of $x, y$ values with an outlier installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = jnp.linspace(-0.7, 0.7, 10)\n",
    "ys = (\n",
    "    -0.2\n",
    "    + 0.4 * xs\n",
    "    + 0.2 * xs**2\n",
    "    + 0.05 * jax.random.normal(key=jax.random.PRNGKey(1), shape=xs.shape)\n",
    ")\n",
    "ys = ys.at[2].set(0.4)\n",
    "\n",
    "Plot.dot({\"x\": xs, \"y\": ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "We use a object called `CurveFit` to get an importance sample. Internally, `CurveFit` is written in terms of the map and switch combinators. That constructor will need simple GenerativeFunctions for the inlier and outlier models. I considered sticking with my design decision before of requiring the user to supply these in terms of the primitive distributions like `genjax.normal`, but instead, since the GFs are so simple, we may was well just write them down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "The alternative might have been to let the user say, for example,\n",
    "```Python\n",
    "lambda y: genjax.normal(y, 0.1)\n",
    "```\n",
    "in the first instance, and that would avoid presenting the decorator, `@`, and a couple of other things this notebook hasn't talked about yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_outlier = genjax.beta(1.0, 1.0)\n",
    "sigma_inlier = genjax.uniform(0.0, 0.3)\n",
    "curve_fit = b.CurveFit(curve=quadratic, sigma_inlier=sigma_inlier, p_outlier=p_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We'll need a function to render the sample from the posterior: since, behind the scenes, Jax has turned the BlockFunctions into BlockFunctions of vectors of parameters, that code will be in terms of `tree_map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior(tr: genjax.Trace, xs: FloatArray, ys: FloatArray):\n",
    "    return (\n",
    "        plot_functions(tr.get_subtrace((\"curve\",)).get_retval(), opacity=0.2)  # type: ignore\n",
    "        + Plot.dot({\"x\": xs, \"y\": ys, \"r\": 4})\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "All that remains is to generate the sample. We select $K$ samples from a posterior categorical distribution taken over $N$ samples. On my home Mac, whose GPU is not accessible to GenJAX, I can get $N=100\\mathrm{k}$ importance samples in a few seconds! Recall that on GenJAX interpreted, each of these took a substantial fraction of second. From there, we can plot, say, $K=100$ of these with alpha blending to visualize the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = curve_fit.importance_sample(xs, ys, 100000, 200)\n",
    "plot_posterior(tr, xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "This is an excellent result, thanks to GenJAX, and I think indicative of what can be done with a DSL to temporarily shift the focus away from the nature of JAX. In this version of the model, the inlier sigma and probability were inference parameters of the model. Let's examine the distributions found by this inference:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "Maybe we can stretch to accommodate a periodic sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_ex(F: b.Block, key=jax.random.PRNGKey(3)):\n",
    "    ys = (\n",
    "        (\n",
    "            0.2 * jnp.sin(4 * xs + 0.3)\n",
    "            + 0.02 * jax.random.normal(key=key, shape=xs.shape)\n",
    "        )\n",
    "        .at[7]\n",
    "        .set(-0.7)\n",
    "    )\n",
    "    fs = b.CurveFit(\n",
    "        curve=F, sigma_inlier=sigma_inlier, p_outlier=p_outlier\n",
    "    ).importance_sample(xs, ys, 100000, 100)\n",
    "    return plot_posterior(fs, xs, ys)\n",
    "\n",
    "\n",
    "periodic_ex(quadratic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Though the sample points are periodic, we supplied the degree 2 polynomial prior, and got reasonable results quickly. Before trying the Periodic prior, we might try degree 3 polynomials, and see what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubic = b.Polynomial(max_degree=3, coefficient_d=genjax.normal(0.0, 1.0))\n",
    "periodic_ex(cubic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "**LGTM!**\n",
    "Now for periodic prior.\n",
    "\n",
    "(NB: your results may vary, but if you see some darker lines this is because the importance sampling is done *with replacement*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "periodic_ex(periodic, key=jax.random.PRNGKey(222))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Interesting! The posterior is full of good solutions but also contains a sprinkling of Nyquist-impostors!\n",
    "\n",
    "...well, it used to. But changing the way the inlier_sigma is now inferred rather than prescribed has changed the quality of the posterior a great deal: Perhaps by allowing small sigmas, we have allowed a trace through which dominates all the others, weight-wise. We can get the old \"curve-cloud\" behavior back by fixing on 0.3 as the inlier sigma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_ex2(F: b.Block):\n",
    "    ys = (0.2 * jnp.sin(4 * xs + 0.3)).at[7].set(-0.7)\n",
    "    fs = b.CurveFit(\n",
    "        curve=periodic,\n",
    "        sigma_inlier=genjax.uniform(0.1, 0.2),\n",
    "        p_outlier=genjax.uniform(0.05, 0.2),\n",
    "    ).importance_sample(xs, ys, 100000, 100)\n",
    "    return plot_posterior(fs, xs, ys)\n",
    "\n",
    "\n",
    "periodic_ex2(quadratic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "I like this framework more than I thought I would when I started it. A wise man once said it is easier to write probabilistic programming languages than probabilistic programs and I found it so. The experience of doing this helped me to understand JAX better. In particular, the idea of creating a custom `Pytree` object seemed exotic to me before I started this, but: if you want to have a Generative Function that produces something other than an array, while retaining full JAX-compatibility, it's exactly what you should do. In this case, the DSL allows the construction and composition of Generative Functions that sample from distributions of real-valued functions on the real line, and that's what curve fitting is about.\n",
    "\n",
    "## Grand Finale: the ascending periodic example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Keeping track of the curve_fit, xs & ys, and other data for different\n",
    "experiments we will conduct can be confusing, so we'll write a little\n",
    "function that yokes the experiment material into a dict.\n",
    "def ascending_periodic_ex(F: b.Block):\n",
    "    xs = jnp.linspace(-0.9, 0.9, 20)\n",
    "    ys = (0.7 * xs + 0.3 * jnp.sin(9 * xs + 0.3)).at[7].set(0.75)\n",
    "    ys += jax.random.normal(key=jax.random.PRNGKey(22), shape=ys.shape) * 0.07\n",
    "    curve_fit = b.CurveFit(curve=F, sigma_inlier=sigma_inlier, p_outlier=p_outlier)\n",
    "    fs = curve_fit.importance_sample(xs, ys, 1000000, 100)\n",
    "    return {\"xs\": xs, \"ys\": ys, \"curve_fit\": curve_fit, \"tr\": fs}\n",
    "\n",
    "\n",
    "ascending_periodic_prior = quadratic + periodic\n",
    "periodic_data = ascending_periodic_ex(ascending_periodic_prior)\n",
    "plot_posterior(periodic_data[\"tr\"], periodic_data[\"xs\"], periodic_data[\"ys\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The posterior distribution here is very thin, suggesting that the priors are too broad (note that I had to increase to 1M samples to get this far, which took 12.6s on my machine). Nonetheless, importance sampling on the sum function was able to find very plausible candidates.\n",
    "\n",
    "NB: actually that used to be true; now the posterior has a lot of interesting things in it (provoked in this instance I think by adding some noise to the y points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_drift(\n",
    "    key: PRNGKey,\n",
    "    curve_fit: b.CurveFit,\n",
    "    tr: genjax.Trace,\n",
    "    scale: ArrayLike = 2.0 / 100.0,\n",
    "    n: int = 1,\n",
    "):\n",
    "    \"\"\"Run `n` steps of the update algorithm. `scale` specifies the amount of gaussian drift in SD units.\"\"\"\n",
    "    if curve_fit.gf != tr.get_gen_fn():\n",
    "        raise ValueError(\"The trace was not generated by the given curve\")\n",
    "    outlier_path = (\"ys\", ..., \"outlier\")\n",
    "\n",
    "    def logit_to_prob(logit):\n",
    "        return 1.0 / (1.0 + jnp.exp(-logit))\n",
    "\n",
    "    def gaussian_drift_step(key: PRNGKey, tr: genjax.Trace):\n",
    "        choices = tr.get_choices()\n",
    "\n",
    "        def update(key: PRNGKey, cm: genjax.ChoiceMap) -> genjax.Trace:\n",
    "            k1, k2 = jax.random.split(key)\n",
    "            updated_tr, weight, arg_diff, bwd_problem = tr.update(k1, cm)\n",
    "            mh_choice = jax.random.uniform(key=k2)\n",
    "            ps = logit_to_prob(weight)\n",
    "            return jax.lax.cond(mh_choice <= ps, lambda: updated_tr, lambda: tr)\n",
    "\n",
    "        def update_coefficients(key: PRNGKey, coefficient_path: tuple):\n",
    "            k1, k2 = jax.random.split(key)\n",
    "            values = choices[coefficient_path]  # pyright: ignore [reportIndexIssue]\n",
    "            drift = values + scale * jax.random.normal(k1, values.shape)\n",
    "\n",
    "            # substitute ... in coefficient path with array of integer indices\n",
    "            def fix(e):\n",
    "                return jnp.arange(len(drift)) if e is Ellipsis else e\n",
    "\n",
    "            new_path = tuple(fix(e) for e in coefficient_path)\n",
    "            cm = C[new_path].set(drift)\n",
    "            return update(k2, cm)\n",
    "\n",
    "        def update_sigma_inlier(key: PRNGKey):\n",
    "            k1, k2 = jax.random.split(key)\n",
    "            s = choices[\"sigma_inlier\"]  # pyright: ignore [reportIndexIssue]\n",
    "            return update(\n",
    "                key, C[\"sigma_inlier\"].set(s + scale * jax.random.normal(key=k1))\n",
    "            )\n",
    "\n",
    "        def update_p_outlier(key: PRNGKey):\n",
    "            # p_outlier is \"global\" to the model. Each point in the curve data\n",
    "            # has an outlier status assignment. We can therefore measure the\n",
    "            # posterior p_outlier and use that data to propose an update.\n",
    "            k1, k2 = jax.random.split(key)\n",
    "            outlier_states = choices[outlier_path]  # pyright: ignore [reportIndexIssue]\n",
    "            n_outliers = jnp.sum(outlier_states)\n",
    "            new_p_outlier = jax.random.beta(\n",
    "                k1,\n",
    "                1.0 + n_outliers,\n",
    "                1 + len(outlier_states) + n_outliers,\n",
    "            )\n",
    "            return update(k2, C[\"p_outlier\"].set(new_p_outlier))\n",
    "\n",
    "        def update_outlier_state(key: PRNGKey):\n",
    "            # we aren't doing this one for two reasons:\n",
    "            # a) there may be a genjax bug (we get a stack trace when we try) and\n",
    "            # b) the instructions (\"update the outlier indicator for each variable\n",
    "            # using an MH proposal that proposes to flip each data point (from inliner\n",
    "            # to outlier, or vice versa) with probability 0.5, or otherwise keeps\n",
    "            # it the same\") is the same thing as just changing all the outlier states\n",
    "            # to a new random 0.5 bernoulli coin flip.\n",
    "\n",
    "            # Secondly, we're changing the _level at which we vmap_ so this code\n",
    "            # below may become obsolete\n",
    "            def to_choicemap(v):\n",
    "                return jax.vmap(\n",
    "                    lambda o: C[\"ys\", jnp.arange(len(o), dtype=int), \"outlier\"].set(o)\n",
    "                )(v)\n",
    "\n",
    "            k1, k2 = jax.random.split(key)\n",
    "            outlier_states = choices[outlier_path]  # pyright: ignore [reportIndexIssue]\n",
    "            flips = jax.random.bernoulli(k1, shape=outlier_states.shape)\n",
    "            return update(\n",
    "                k2,\n",
    "                to_choicemap(jnp.logical_xor(outlier_states, flips).astype(int)),\n",
    "            )\n",
    "\n",
    "        k1, k2, *ks = jax.random.split(key, 2 + len(curve_fit.coefficient_paths))\n",
    "        for k, path in zip(ks, curve_fit.coefficient_paths):\n",
    "            tr = update_coefficients(k, path)\n",
    "        tr = update_p_outlier(k1)\n",
    "        tr = update_sigma_inlier(k2)\n",
    "        # tr = update_outlier_state(k3)\n",
    "        return tr\n",
    "\n",
    "    # The first step is an empty update to stabilize the type of the trace (GEN-306)\n",
    "    tr0 = jax.vmap(lambda t: t.update(jax.random.PRNGKey(0), C.d({})))(tr)[0]\n",
    "    # The blocks library arranges for importance samples to come from vmap (even if\n",
    "    # only one was requested), so we may expect a normally-scalar field like score\n",
    "    # to have a leading axis which equals the number of traces within.\n",
    "    K = tr.get_score().shape[0]  # pyright: ignore [reportAttributeAccessIssue]\n",
    "    # This is a two-dimensional JAX operation: we scan through `n` steps of `K` traces\n",
    "    sub_keys = jax.random.split(key, (n, K))\n",
    "    tr, _ = jax.lax.scan(\n",
    "        lambda trace, keys: (jax.vmap(gaussian_drift_step)(keys, trace), None),\n",
    "        tr0,\n",
    "        sub_keys,\n",
    "    )\n",
    "    return tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "In this cell, we will take the original curve fit size-200 importance sample of\n",
    "the degree-2 polynomial prior and run it through 100 steps of gaussian drift.\n",
    "key, sub_key = jax.random.split(jax.random.PRNGKey(314159))\n",
    "tru = gaussian_drift(sub_key, curve_fit, tr, n=100)\n",
    "plot_posterior(tru, xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now let's try something more difficult. We will gaussian drift the\n",
    "periodic example.\n",
    "key, sub_key = jax.random.split(key)\n",
    "periodic_t1 = gaussian_drift(\n",
    "    sub_key, periodic_data[\"curve_fit\"], periodic_data[\"tr\"], n=100\n",
    ")\n",
    "plot_posterior(periodic_t1, periodic_data[\"xs\"], periodic_data[\"ys\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Let's drift the _result_ of that experiment at a smaller scale and see if that helps\n",
    "key, sub_key = jax.random.split(key)\n",
    "periodic_t2 = gaussian_drift(\n",
    "    sub_key, periodic_data[\"curve_fit\"], periodic_t1, n=100, scale=0.005\n",
    ")\n",
    "plot_posterior(periodic_t2, periodic_data[\"xs\"], periodic_data[\"ys\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genjax-RWVhKTPb-py3.12",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
